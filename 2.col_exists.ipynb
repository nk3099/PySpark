{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2243fcb-1943-4085-8440-e5dfbfcc4d3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. checking if column exists in a dataframe\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import *\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df8e141e-0047-4cab-b3d3-142532820712",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>1</td><td>neeraj</td><td>male</td><td>2000</td></tr><tr><td>2</td><td>nitin</td><td>male</td><td>5000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "neeraj",
         "male",
         2000
        ],
        [
         2,
         "nitin",
         "male",
         5000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+\n| id|  name|gender|salary|\n+---+------+------+------+\n|  1|neeraj|  male|  2000|\n|  2| nitin|  male|  5000|\n+---+------+------+------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data=[(1,\"neeraj\",'male',2000),(2,\"nitin\",'male',5000)]\n",
    "cols=['id','name','gender','salary']\n",
    "\n",
    "df=spark.createDataFrame(data,cols)\n",
    "df=spark.createDataFrame(data=data,schema=cols)\n",
    "\n",
    "display(df)\n",
    "df.show()\n",
    "\n",
    "df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4c3c00b-709c-4d85-803c-9002066db98a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on StructType in module pyspark.sql.types object:\n\nclass StructType(DataType)\n |  StructType(fields: Optional[List[pyspark.sql.types.StructField]] = None)\n |  \n |  Struct type, consisting of a list of :class:`StructField`.\n |  \n |  This is the data type representing a :class:`Row`.\n |  \n |  Iterating a :class:`StructType` will iterate over its :class:`StructField`\\s.\n |  A contained :class:`StructField` can be accessed by its name or position.\n |  \n |  Examples\n |  --------\n |  >>> from pyspark.sql.types import *\n |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n |  >>> struct1[\"f1\"]\n |  StructField('f1', StringType(), True)\n |  >>> struct1[0]\n |  StructField('f1', StringType(), True)\n |  \n |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n |  >>> struct1 == struct2\n |  True\n |  >>> struct1 = StructType([StructField(\"f1\", CharType(10), True)])\n |  >>> struct2 = StructType([StructField(\"f1\", CharType(10), True)])\n |  >>> struct1 == struct2\n |  True\n |  >>> struct1 = StructType([StructField(\"f1\", VarcharType(10), True)])\n |  >>> struct2 = StructType([StructField(\"f1\", VarcharType(10), True)])\n |  >>> struct1 == struct2\n |  True\n |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n |  >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n |  ...     StructField(\"f2\", IntegerType(), False)])\n |  >>> struct1 == struct2\n |  False\n |  \n |  The below example demonstrates how to create a DataFrame based on a struct created\n |  using class:`StructType` and class:`StructField`:\n |  \n |  >>> data = [(\"Alice\", [\"Java\", \"Scala\"]), (\"Bob\", [\"Python\", \"Scala\"])]\n |  >>> schema = StructType([\n |  ...     StructField(\"name\", StringType()),\n |  ...     StructField(\"languagesSkills\", ArrayType(StringType())),\n |  ... ])\n |  >>> df = spark.createDataFrame(data=data, schema=schema)\n |  >>> df.printSchema()\n |  root\n |   |-- name: string (nullable = true)\n |   |-- languagesSkills: array (nullable = true)\n |   |    |-- element: string (containsNull = true)\n |  >>> df.show()\n |  +-----+---------------+\n |  | name|languagesSkills|\n |  +-----+---------------+\n |  |Alice|  [Java, Scala]|\n |  |  Bob|[Python, Scala]|\n |  +-----+---------------+\n |  \n |  Method resolution order:\n |      StructType\n |      DataType\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __getitem__(self, key: Union[str, int]) -> pyspark.sql.types.StructField\n |      Access fields by name or slice.\n |  \n |  __init__(self, fields: Optional[List[pyspark.sql.types.StructField]] = None)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __iter__(self) -> Iterator[pyspark.sql.types.StructField]\n |      Iterate the fields\n |  \n |  __len__(self) -> int\n |      Return the number of fields.\n |  \n |  __repr__(self) -> str\n |      Return repr(self).\n |  \n |  add(self, field: Union[str, pyspark.sql.types.StructField], data_type: Union[str, pyspark.sql.types.DataType, NoneType] = None, nullable: bool = True, metadata: Optional[Dict[str, Any]] = None) -> 'StructType'\n |      Construct a :class:`StructType` by adding new elements to it, to define the schema.\n |      The method accepts either:\n |      \n |          a) A single parameter which is a :class:`StructField` object.\n |          b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n |             metadata(optional). The data_type parameter may be either a String or a\n |             :class:`DataType` object.\n |      \n |      Parameters\n |      ----------\n |      field : str or :class:`StructField`\n |          Either the name of the field or a :class:`StructField` object\n |      data_type : :class:`DataType`, optional\n |          If present, the DataType of the :class:`StructField` to create\n |      nullable : bool, optional\n |          Whether the field to add should be nullable (default True)\n |      metadata : dict, optional\n |          Any additional metadata (default None)\n |      \n |      Returns\n |      -------\n |      :class:`StructType`\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql.types import IntegerType, StringType, StructField, StructType\n |      >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n |      ...     StructField(\"f2\", StringType(), True, None)])\n |      >>> struct1 == struct2\n |      True\n |      >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n |      >>> struct1 == struct2\n |      True\n |      >>> struct1 = StructType().add(\"f1\", \"string\", True)\n |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n |      >>> struct1 == struct2\n |      True\n |  \n |  fieldNames(self) -> List[str]\n |      Returns all field names in a list.\n |      \n |      Examples\n |      --------\n |      >>> from pyspark.sql.types import StringType, StructField, StructType\n |      >>> struct = StructType([StructField(\"f1\", StringType(), True)])\n |      >>> struct.fieldNames()\n |      ['f1']\n |  \n |  fromInternal(self, obj: Tuple) -> 'Row'\n |      Converts an internal SQL object into a native Python object.\n |  \n |  jsonValue(self) -> Dict[str, Any]\n |  \n |  needConversion(self) -> bool\n |      Does this type needs conversion between Python object and internal SQL object.\n |      \n |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n |  \n |  simpleString(self) -> str\n |  \n |  toInternal(self, obj: Tuple) -> Tuple\n |      Converts a Python object into an internal SQL object.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  fromJson(json: Dict[str, Any]) -> 'StructType' from builtins.type\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from DataType:\n |  \n |  __eq__(self, other: Any) -> bool\n |      Return self==value.\n |  \n |  __hash__(self) -> int\n |      Return hash(self).\n |  \n |  __ne__(self, other: Any) -> bool\n |      Return self!=value.\n |  \n |  json(self) -> str\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from DataType:\n |  \n |  typeName() -> str from builtins.type\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from DataType:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n"
     ]
    }
   ],
   "source": [
    "help(df.schema) #help-> to see documentation of function\n",
    "\n",
    "#object of StrucType that will hold all the column information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b4950bb-d638-4640-8260-70ddbf6d3b66",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('id', LongType(), True), StructField('name', StringType(), True), StructField('gender', StringType(), True), StructField('salary', LongType(), True)])\n"
     ]
    }
   ],
   "source": [
    "print(df.schema) #StructType(list(StructField(id,LongType,true),..] -> 'true' stands for nullbable or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5b91d5e-8e61-4b67-bf26-19971dc6314d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fieldNames in module pyspark.sql.types:\n\nfieldNames() -> List[str] method of pyspark.sql.types.StructType instance\n    Returns all field names in a list.\n    \n    Examples\n    --------\n    >>> from pyspark.sql.types import StringType, StructField, StructType\n    >>> struct = StructType([StructField(\"f1\", StringType(), True)])\n    >>> struct.fieldNames()\n    ['f1']\n\n"
     ]
    }
   ],
   "source": [
    "help(df.schema.fieldNames) #gives all columnNames in List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "024656c1-af40-4579-8191-7d5e77c2afce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'name', 'gender', 'salary']\n"
     ]
    }
   ],
   "source": [
    "print(df.schema.fieldNames()) #fieldNames() is a function -> gives all columnNames in a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e34aabb-2787-4d04-872c-3cf8bc6099db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "fields_df = df.schema.fieldNames()\n",
    "count_df = fields_df.count('id')\n",
    "print(count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f1e6c6-d8c0-4b77-bbe2-a49f528d9dad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id columns is present in dataframe\n"
     ]
    }
   ],
   "source": [
    "#Logic to check if Column Exists:\n",
    "\n",
    "fields_df = df.schema.fieldNames()\n",
    "if fields_df.count('id') > 0:\n",
    "    print('id column is present in dataframe')\n",
    "else:\n",
    "    print('id column is not present in dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1227e0df-518d-4413-8e15-077f89e752ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id column is present in dataframe\n"
     ]
    }
   ],
   "source": [
    "#Logic to check if Column Exists:\n",
    "\n",
    "fields_df = df.schema.fieldNames()\n",
    "if fields_df.count(fields_df[0]) > 0:\n",
    "    print(f'{fields_df[0]} column is present in dataframe')\n",
    "else:\n",
    "    print(f'{fields_df[0]} column is not present in dataframe')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.col_exists",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
