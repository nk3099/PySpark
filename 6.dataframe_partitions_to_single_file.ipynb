{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "726bdfbe-3bbb-4e98-a88f-8076d44d881c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6.to write Dataframe as single file with specific name in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c8fc08f-8982-4051-82c2-778d3f76116c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+------+\n| id|        name|gender|salary|\n+---+------------+------+------+\n|  1|neeraj kumar|  male|  2000|\n|  2| nitin kumar|  male|  3000|\n+---+------------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data = [(1,'neeraj kumar','male',2000),(2,'nitin kumar','male',3000)]\n",
    "schema = ['id','name','gender','salary']\n",
    "\n",
    "df = spark.createDataFrame(data,schema)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cc285f5-e4f3-4413-abc2-2be7d9b68c3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df .write.csv('/FileStore/personsoriginal/',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ee1bda-998e-403f-a159-61cdb0440c82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+------+\n| id|        name|gender|salary|\n+---+------------+------+------+\n|  1|neeraj kumar|  male|  2000|\n|  2| nitin kumar|  male|  3000|\n+---+------------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('/FileStore/personsoriginal/',header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbb7111a-a716-414b-969f-1fe6c65c991d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#coalesce or #repartition -- to write as a single partition\n",
    "\n",
    "df.coalesce(1).write.csv('/FileStore/personstemp/',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d03b528-0ed9-40f9-8c2c-ff80d153a072",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div class = \"ansiout\"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in\n",
       "this package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or\n",
       "another FileSystem URI.\n",
       "\n",
       "For more info about a method, use <b>dbutils.fs.help(\"methodName\")</b>.\n",
       "\n",
       "In notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps\n",
       "straightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\"\n",
       "translates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\".\n",
       "    <h3>mount</h3><b>mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -> Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -> Deletes a DBFS mount point<br /><b>updateMount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Similar to mount(), but updates an existing mount point (if present) instead of creating a new one<br /><br /><h3>fsutils</h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -> Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -> Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -> Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -> Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -> Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -> Removes a file or directory<br /><br /></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class = \"ansiout\"><b>dbutils.fs</b> provides utilities for working with FileSystems. Most methods in\nthis package can take either a DBFS path (e.g., \"/foo\" or \"dbfs:/foo\"), or\nanother FileSystem URI.\n\nFor more info about a method, use <b>dbutils.fs.help(\"methodName\")</b>.\n\nIn notebooks, you can also use the %fs shorthand to access DBFS. The %fs shorthand maps\nstraightforwardly onto dbutils calls. For example, \"%fs head --maxBytes=10000 /file/path\"\ntranslates into \"dbutils.fs.head(\"/file/path\", maxBytes = 10000)\".\n    <h3>mount</h3><b>mount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Mounts the given source directory into DBFS at the given mount point<br /><b>mounts: Seq</b> -> Displays information about what is mounted within DBFS<br /><b>refreshMounts: boolean</b> -> Forces all machines in this cluster to refresh their mount cache, ensuring they receive the most recent information<br /><b>unmount(mountPoint: String): boolean</b> -> Deletes a DBFS mount point<br /><b>updateMount(source: String, mountPoint: String, encryptionType: String = \"\", owner: String = null, extraConfigs: Map = Map.empty[String, String]): boolean</b> -> Similar to mount(), but updates an existing mount point (if present) instead of creating a new one<br /><br /><h3>fsutils</h3><b>cp(from: String, to: String, recurse: boolean = false): boolean</b> -> Copies a file or directory, possibly across FileSystems<br /><b>head(file: String, maxBytes: int = 65536): String</b> -> Returns up to the first 'maxBytes' bytes of the given file as a String encoded in UTF-8<br /><b>ls(dir: String): Seq</b> -> Lists the contents of a directory<br /><b>mkdirs(dir: String): boolean</b> -> Creates the given directory if it does not exist, also creating any necessary parent directories<br /><b>mv(from: String, to: String, recurse: boolean = false): boolean</b> -> Moves a file or directory, possibly across FileSystems<br /><b>put(file: String, contents: String, overwrite: boolean = false): boolean</b> -> Writes the given String out to a file, encoded in UTF-8<br /><b>rm(dir: String, recurse: boolean = false): boolean</b> -> Removes a file or directory<br /><br /></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.help()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c9f41e3-cf29-436c-aa6c-93f789bf196b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FileInfo(path='dbfs:/FileStore/personstemp/_SUCCESS', name='_SUCCESS', size=0, modificationTime=1729323667000), FileInfo(path='dbfs:/FileStore/personstemp/_committed_8010333200259552195', name='_committed_8010333200259552195', size=113, modificationTime=1729323667000), FileInfo(path='dbfs:/FileStore/personstemp/_started_8010333200259552195', name='_started_8010333200259552195', size=0, modificationTime=1729323666000), FileInfo(path='dbfs:/FileStore/personstemp/part-00000-tid-8010333200259552195-bffc5b6d-d714-46ba-9b45-5deea56586d2-158-1-c000.csv', name='part-00000-tid-8010333200259552195-bffc5b6d-d714-46ba-9b45-5deea56586d2-158-1-c000.csv', size=71, modificationTime=1729323667000)]\n"
     ]
    }
   ],
   "source": [
    "filenames = dbutils.fs.ls('/FileStore/personstemp/')\n",
    "print(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c99f558-ec09-4d54-a41d-29744a8c3228",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "####1.identify the dataframe data(the single part-file (using coalesce) with specified name)from \"personstemp\" temporary location\n",
    "\n",
    "####2. save/stored file into permanent location as a part-file with specific name\n",
    "\n",
    "\n",
    "####3. remove the temporary location compeletely\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d2e9312-1296-4642-8e23-0898cda26deb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-tid-8010333200259552195-bffc5b6d-d714-46ba-9b45-5deea56586d2-158-1-c000.csv\nOut[47]: True"
     ]
    }
   ],
   "source": [
    "#filenames = dbutils.fs.ls('/FileStore/personstemp/')\n",
    "\n",
    "name=\" \"\n",
    "\n",
    "for filename in filenames:\n",
    "    if filename.name.endswith(\".csv\"):\n",
    "        name=filename.name\n",
    "\n",
    "print(name)\n",
    "\n",
    "dbutils.fs.cp('/FileStore/personstemp/' + name, '/FileStore/personsdata/persons.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faf52e67-0f13-4aeb-ad64-f197996d94a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+------+------+\n| id|        name|gender|salary|\n+---+------------+------+------+\n|  1|neeraj kumar|  male|  2000|\n|  2| nitin kumar|  male|  3000|\n+---+------------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(\"/FileStore/personsdata/persons.csv\",header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852d6d6c-7efd-415c-8895-2ed9406d2806",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[50]: True"
     ]
    }
   ],
   "source": [
    "dbutils.fs.rm(\"/FileStore/personstemp/\", recurse=True) #recursively remove all files & folders from the path"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "6.dataframe_partitions_to_single_file",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
